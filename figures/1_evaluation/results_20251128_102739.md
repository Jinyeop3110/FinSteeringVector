# FinQA Experiment Results

**Model:** Qwen/Qwen2.5-1.5B-Instruct
**Date:** 2025-11-28 10:27
**Samples:** 256
**Seeds:** 4 per configuration

## Results Table

| Config | Shots | Accuracy | Std | Latency (s) | Input Tokens | Output Tokens |
|--------|-------|----------|-----|-------------|--------------|---------------|
| Vanilla | 0 | 8.39% | 0.00% | 6.9 | 1179 | 6 |
| CoT | 0 | 29.37% | 0.00% | 68.7 | 1215 | 194 |
| CoT | 1 | 31.37% | 1.12% | 77.8 | 2566 | 193 |
| CoT | 2 | 32.01% | 1.08% | 86.8 | 3918 | 190 |
| CoT | 3 | 32.08% | 0.91% | 93.3 | 5257 | 191 |
| CoT | 4 | 30.00% | 0.85% | 97.1 | 6605 | 188 |

## Key Findings

1. **CoT vs Vanilla:** Chain-of-thought prompting significantly improves accuracy
2. **ICL Scaling:** Accuracy generally improves with more examples up to a saturation point
3. **Latency Trade-off:** More examples increase input tokens but latency is dominated by output generation
