# FinQA Experiment Results

**Model:** Qwen/Qwen2.5-1.5B-Instruct
**Date:** 2025-11-27 18:46
**Samples:** 256
**Seeds:** 4 per configuration

## Results Table

| Config | Shots | Accuracy | Std | Latency (s) | Input Tokens | Output Tokens |
|--------|-------|----------|-----|-------------|--------------|---------------|
| Vanilla | 0 | 8.39% | 0.00% | 6.9 | 1179 | 6 |
| CoT | 0 | 29.37% | 0.00% | 69.2 | 1215 | 194 |
| CoT | 1 | 31.53% | 0.78% | 72.3 | 1518 | 199 |
| CoT | 2 | 30.13% | 0.81% | 76.5 | 1808 | 201 |
| CoT | 3 | 30.42% | 0.61% | 77.1 | 2095 | 198 |
| CoT | 4 | 31.24% | 0.79% | 75.9 | 2385 | 194 |
| CoT | 5 | 31.41% | 0.95% | 73.2 | 2672 | 186 |

## Key Findings

1. **CoT vs Vanilla:** Chain-of-thought prompting significantly improves accuracy
2. **ICL Scaling:** Accuracy generally improves with more examples up to a saturation point
3. **Latency Trade-off:** More examples increase input tokens but latency is dominated by output generation
