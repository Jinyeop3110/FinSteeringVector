# FinQA Experiment Results

**Model:** Qwen/Qwen2.5-1.5B-Instruct
**Date:** 2025-11-27 21:01
**Samples:** 256
**Seeds:** 4 per configuration

## Results Table

| Config | Shots | Accuracy | Std | Latency (s) | Input Tokens | Output Tokens |
|--------|-------|----------|-----|-------------|--------------|---------------|
| Vanilla | 0 | 8.39% | 0.00% | 6.9 | 1179 | 6 |
| CoT | 0 | 25.41% | 0.00% | 75.4 | 1168 | 215 |
| CoT | 1 | 26.52% | 1.05% | 75.0 | 1436 | 210 |
| CoT | 2 | 26.05% | 0.69% | 73.8 | 1698 | 200 |
| CoT | 3 | 25.52% | 0.88% | 78.4 | 1954 | 200 |
| CoT | 4 | 26.22% | 1.16% | 80.8 | 2210 | 200 |
| CoT | 5 | 26.05% | 1.18% | 76.4 | 2468 | 192 |

## Key Findings

1. **CoT vs Vanilla:** Chain-of-thought prompting significantly improves accuracy
2. **ICL Scaling:** Accuracy generally improves with more examples up to a saturation point
3. **Latency Trade-off:** More examples increase input tokens but latency is dominated by output generation
